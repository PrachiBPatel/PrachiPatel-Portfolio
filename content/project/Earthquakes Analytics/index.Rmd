---
title: "Earthquake Analysis"
author: "Prachi Patel"
date: "2022-09-28"
excerpt: "This project aims to analyze earthquake data to understand patterns, trends, and characteristics of seismic activity. By examining factors such as location, magnitude, and depth of earthquakes over time, we seek to gain insights into seismic behavior and potential risk areas. Through data visualization and statistical analysis, we aim to contribute to earthquake preparedness and mitigation efforts."
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options: 
  markdown: 
    wrap: sentence

---

### Introduction

This project aims to analyze earthquake data to understand patterns, trends, and characteristics of seismic activity. By examining factors such as location, magnitude, and depth of earthquakes over time, we seek to gain insights into seismic behavior and potential risk areas. Through data visualization and statistical analysis, we aim to contribute to earthquake preparedness and mitigation efforts.

### Data Dictionary

```{r echo=FALSE, results='asis'}
options(knitr.kable.NA = '')
data.frame(Variable = c(""),
           Description = c("")) |>
  knitr::kable()
```

### Import Library

This section loads and attaches all the necessary libraries for this project.

```{r, warning=FALSE}
library(reticulate)
```

```{python, include=FALSE}
# !pip install matplot
```

### Import python Libraries

Adding the necessary python libraries for this project.

```{python}
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
```

### Import Dataset

```{python}
df1 = pd.read_csv("earthquakes-dataset.csv")
df1.head()
```

I add the `errors` argument with the value `coerce` to make sure that if there's any problem parsing the data, it will just set that bad value to null instead of stopping everything. 

This way, the process keeps going smoothly even if there are a few mistakes in the data.

```{python}
df1['Datetime'] = pd.to_datetime(df1['Date']+' '+df1['Time'],errors='coerce')
df1.head()
```

Set the `Datetime` field as the index to enable more efficient and meaningful data analysis.

```{python, warning=FALSE}
df1 = pd.read_csv("earthquakes-dataset.csv",index_col=0, parse_dates=[['Date','Time']])
df1.index = pd.to_datetime(df1.index, errors='coerce')
df1.head()
```

Data type of the `Depth` field is currently set to float, I have changed it to int.

```{python}
df1['Depth'] = df1['Depth'].astype(int)
df1.dtypes
```

Drop all the columns with the null values.

```{python}
df1 = df1.dropna(axis = 1)
df1.head()
```

Verify if any null value is available or not.

```{python}
df1.info() # As we see no null values are available.
```

Created a new dataset having important columns such as latitude, longitude, type, depth and magnitude.

```{python}
df2 = df1[['Latitude','Longitude','Type','Depth','Magnitude']]
```

Sample is used to get the random records based on provided number of records.

```{python}
df2.sample(5)
```

Now, let's check biggest earthquake in the last fifty years.

```{python}
df2[df2.Type == 'Earthquake'].Magnitude.max()
df2[df2.Type == 'Earthquake'].Depth.max()
df2[df2.Type == 'Earthquake'].Depth.min()
```

let's narrow down the dataset to include only the records corresponding to the largest earthquake event.

```{python}
df2.loc[df2['Magnitude'] == 9.1]
df2.loc[df2['Depth'] == 700]
df2.loc[df2['Depth'] == -1]
```

Added a visualization for the distribution of earthquake magnitudes in the dataset, as it will provides insights into the frequency and range of earthquake magnitudes.

```{python}
plt.hist(df2['Magnitude'], bins=20, edgecolor='black')
plt.xlabel('Magnitude')
plt.ylabel('Number of Earthquakes')
plt.title('Distribution of Earthquake Magnitudes (1965-2016)')
plt.grid(True)
plt.show()
```

*Bins are the buckets into which data is grouped. Typically, the number of bins is chosen between 5 and 10, but the optimal number can vary depending on the specifics of your dataset.*


Based on the graph we can see that higher magnitudes are not the highest one.

```{python, warning=FALSE, include=FALSE}
# Let's visualizes the line chart to show the biggest earthquake each year changes over time.
plt.plot(df2['Magnitude'][df2.Type == 'Earthquake'].resample('6M').max())
plt.xlabel('Year')
plt.ylabel('Maximum Magnitude')
plt.title('Maximum Earthquake Magnitude Each year')
plt.grid(True)
plt.show()
# And here we see that there is no such fluctuations in the maximum magnitudes.
```

-------------------------------------------------------------------------------------

To determine how many earthquakes occur per year, we need to use unique indexes. I have used bar plots for this analysis as they allow us to compare the number of earthquakes on a year-by-year basis.

-----------------------------------------------------------------------------------

Let's check the data frame to identify if any data is causing an error or not.

```{python}
# Check for missing years in the index
missing_years = df2.index.year[df2['Type'] == 'Earthquake'].value_counts().sort_index()

# Check if there are any missing years
if missing_years.empty:
    print("There are no missing years in the earthquake data.")
else:
    df2.loc[df2.index.year.isnull()]

```

These three records are causing an error because we have set the date_time column as the index, and these records have improperly formatted values. Therefore, we need to delete these records and then recreate the index for the date_time column.

```{python}
df2 = df2.reset_index().dropna().set_index('Date_Time')
plt.bar(df2.index.year.unique(),df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index())
plt.title('Earthquake per year')
plt.show()
```

Let's check how many Nuclear Explosions we have per year?

```{python}
plt.bar(df2[df2.Type == 'Nuclear Explosion'].index.year.unique(),
        df2.index.year[df2.Type == 'Nuclear Explosion'].value_counts().sort_index())
plt.title('Nukes per Year')
plt.show()
```

Scatter Plots - to see relationships and co-relation between different values.

```{python}
# Filter earthquake data and select relevant columns
earthquake_data = df2[df2['Type'] == 'Earthquake']
earthquake_data = earthquake_data[['Latitude', 'Longitude', 'Depth', 'Magnitude']]

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(earthquake_data['Longitude'], earthquake_data['Latitude'], c=earthquake_data['Depth'], cmap='viridis', alpha=0.7)
plt.colorbar(label='Depth')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Earthquake Locations and Depths')
plt.grid(True)
plt.show()
```

```{python}
# Define the size of the figure
plt.figure(figsize=(12, 8))

# Create the scatter plot
plt.scatter(df2['Longitude'], df2['Latitude'], 
            s=df2['Magnitude'] * 50, 
            c=df2['Depth'], cmap='viridis', alpha=0.7)

# Add color bar to represent depth
plt.colorbar(label='Depth (km)')

# Set labels and title
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Earthquake Locations')

# Show the plot
plt.grid(True)
plt.show()
```

Depth of Each Earthquakes

```{python}
# Filter earthquake data
earthquake_data = df2[df2['Type'] == 'Earthquake']

# Define figure size
plt.figure(figsize=(12, 8))

# Create scatter plot
plt.scatter(earthquake_data.index.year, earthquake_data['Depth'], 
            s=earthquake_data['Magnitude'] * 10, 
            c='green', alpha=0.7)

# Add labels and title
plt.xlabel('Year')
plt.ylabel('Depth (km)')
plt.title('Depth of Earthquakes Over Time')

# Add legend for magnitude
for area in [10, 50, 100]:
    plt.scatter([], [], c='green', s=area, label=str(area))
plt.legend(scatterpoints=1, title='Magnitude')

# Show plot
plt.grid(True)
plt.show()
```

