---
title: "Earthquake Analysis"
author: "Prachi Patel"
date: "2022-09-28"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
# layout options: single or single-sidebar
layout: single
categories:
  - Theme Features
  - R
  - package
---

```{r}
library(reticulate)
```

```{python}
# !pip install matplot
```


```{python}
import numpy as np
import pandas as pd
```


```{python}
df1 = pd.read_csv("earthquakes-dataset.csv")
df1.head()
```

if you add errors argument as coerce then it will make sure that invalid parsing will resulting as a null value for that specific record rather than just failing.

```{python}
df1['Datetime'] = pd.to_datetime(df1['Date']+' '+df1['Time'],errors='coerce')
df1.head()
```

```{python}
df1['Datetime'][0]
```

a Weekday as Sun, Mon

%A Weekday as full name as Sunday, Monday
%w Weekday as decimal no as 0,1,2...
%d Day of month as 01,02
%b Months as Jan, Feb
%B Months as January, February
%m Months as 01,02
%y Year without century as 11,12,13
%Y Year with century 2011,2012
%H 24 Hours clock from 00 to 23
%I 12 Hours clock from 01 to 12
%p AM, PM
%M Minutes from 00 to 59
%S Seconds from 00 to 59
%f Microseconds 6 decimal numbers

```{python}
df1['Datetime'][0].strftime('%A or %a and in %B')
```

```{python}
df1 = df1.set_index(['Datetime'])
df1.head()
```

```{python}
df1.dtypes
```

```{python}
df1['Depth'] = df1['Depth'].astype(int)
df1.dtypes
```

Read csv function has additional arguments like parse date, make an index.
Steps we have done above this to merge date and time column and make that column as a index column.
We can do the same process in the line of code while adding the csv file in to the jupyter notebook.

```{python}
df1 = pd.read_csv("earthquakes-dataset.csv",index_col=0, parse_dates=[['Date','Time']])
df1.head()
```

```{python}
df1.index
```

```{python}
df1.index = pd.to_datetime(df1.index, errors='coerce')
df1.index
```

info function to check the null values we have.

```{python}
df1.info()
```

Drop all the columns with the null values.

```{python}
df1 = df1.dropna(axis = 1)
df1.head()
```

```{python}
df1.info()
```

Creaded a new dataset having improtant columns such as latitude, longitude, type, depth and magnitude.

```{python}
df2 = df1[['Latitude','Longitude','Type','Depth','Magnitude']]
```

Sample is used to get the random records based on provided number of records.

```{python}
df2.sample(5)
```

Get the unique values for specific columns.

```{python}
df2.Type.unique()
```

Biggest Earthquack in the last fifty years.

```{python}
df2[df2.Type == 'Earthquake'].Magnitude.max()
```

```{python}
df2[df2.Type == 'Earthquake'].Depth.max()
```

```{python}
df2[df2.Type == 'Earthquake'].Depth.min()
```

Value count for each value present in the type column.

```{python}
df2.Type.value_counts()
```

Average range of magnitude for the specified period of time.

```{python}
df2['Magnitude'][df2.Type == 'Earthquacke'].resample('1D').mean()
```

Check stand deviation.

```{python}
df2['Magnitude'][df2.Type == 'Earthquacke'].resample('1M').std()
```

```{python}
df2.loc[df2['Magnitude'] == 9.1]
```

```{python}
df2.loc[df2['Depth'] == -1.1]
```

```{python}
df2.index.year[df2.Type == 'Nuclear Explosion'].value_counts().sort_index()
```

To Install packages in Jupyter Notebook you can use conda or pip install.
Ex.,
conda install matplotlib
or
pip install matplotlib
or
!pip install matplotlib

```{python}
import matplotlib as mpl
import matplotlib.pyplot as plt
```

inline is used to enable static images of the plots into the notebook environment.

```{python}
'%matplotlib inlin'
```

```{python}
plt.hist(df2['Magnitude'])
```

```{python}
plt.hist(df2['Magnitude'])
plt.xlabel('Magnitude')
plt.ylabel('Number of Earthquackes')
plt.title('1965-2016 Earthquackes')
```

bins are the vuckets where the data is relied into.
Normally choose bins size between 5 to 10. But it also depends upon your database as well.

```{python}
plt.hist(df2['Magnitude'], bins = 10, edgecolor = 'black')
plt.xlabel('Magnitude')
plt.ylabel('Number of Earthquackes')
plt.title('1965-2016 Earthquackes')
```

```{python}
plt.hist(df2['Magnitude'], bins=[6,6.1,6.2,6.3,6.4,6.5,6.6,6.7,6.8,6.9,7], edgecolor = 'black')
plt.xlabel('Magnitude')
plt.ylabel('Number of Earthquackes')
plt.title('1965-2016 Earthquackes')
```

```{python}
plt.hist(df2['Magnitude'], bins=10, edgecolor = 'black', range = [6,7])
plt.xlabel('Magnitude')
plt.ylabel('Number of Earthquackes')
plt.title('1965-2016 Earthquackes')
```

If you want the same graph with different bins value then you can create bins function and just change the value of bins using function instead of writing whole line of code again and again.

```{python}
def myplot(bins):
    plt.hist(df2['Magnitude'], bins = bins, edgecolor = 'black')
    plt.xlabel('Magnitude')
    plt.ylabel('Number of Earthquackes')
    plt.title('1965-2016 Earthquackes')
    
myplot(10)
```

```{python}
myplot(20)
```

Line Chart

```{python}
plt.plot(df2['Magnitude'][df2.Type == 'Earthquake'].resample('1Y').max())
```

```{python}
plt.plot(df2['Magnitude'][df2.Type == 'Earthquake'].resample('6M').max())
```

How many earthquakes we have per year?
For this we have to use unique indexes and here we are using bar plots because we are comparing number year by year.

```{python}
# plt.bar(df2.index.year.unique(),df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index())
# plt.title('Earthquake per year')
```

Let's check the data frame that which data is causing this error.

```{python}
df2.index.year.unique().shape
```

```{python}
df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index().shape
```

```{python}
# f2.index.year.isnull().sum()
```

```{python}
df2.loc[df2.index.year.isnull()]
```

```{python}
df2.loc[df2.index.year.isnull()]
```

These three records are causing an error because we have make date time column as index and these three records have not proper formatted value.
so, first delete these values and then once again create an index for date_time column.

```{python}
df2 = df2.reset_index().dropna().set_index('Date_Time')
plt.bar(df2.index.year.unique(),df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index())
plt.title('Earthquake per year')
```

How many Nuclear Explosions we have per year?

```{python}
plt.bar(df2[df2.Type == 'Nuclear Explosion'].index.year.unique(),
        df2.index.year[df2.Type == 'Nuclear Explosion'].value_counts().sort_index())
plt.title('Nukes per Year')
```

Scatter Plots - to see relationships and co-relation between different values.

```{python}
#plt.scatter(df2[df2.Type == 'Earthquake'].index.year.unique(),
#        df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index())
#plt.title('Earthquake per Year')
```

```{python}
# plt.scatter(df2[df2.Type == 'Earthquake'].index.year.unique(),
#         df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index(), df2['Depth'], 'green', alpha=0.7)
```

```{python}
plt.scatter(df2['Magnitude'],df2['Depth'])
```

```{python}
plt.figure(figsize=(19,10))
plt.scatter(df2['Longitude'],df2['Latitude'],df2['Magnitude']*10,df2['Depth'])
```

Depth of Each Earthquakes

```{python}
# plt.scatter(df2[df2.Type == 'Earthquake'].index.year.unique(),
#            df2.index.year[df2.Type == 'Earthquake'].value_counts().sort_index(),df2['Depth'],'green',alpha = 0.7)
# area = df2['Depth']
# for area in [10,50,100]:
#     plt.scatter([],[],c='k',s=area,label=str(area))
# plt.ledgend(scatterpoints=1, title='I am legend', loc = 'lower center')
```

