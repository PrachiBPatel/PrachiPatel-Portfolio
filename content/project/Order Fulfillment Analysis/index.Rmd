---
title: "Order Fulfillment Analysis"
author: "Prachi Patel"
date: "2022-11-27"
excerpt: "This project aims to analyze the factors affecting the on-time delivery of orders for a major mail-order company. By employing logistic regression, Naive-Bayes classification, and Linear Discriminant Analysis, the goal is to identify significant predictors of delivery success and compare the accuracy, processing speed, and false positive rates of these classification techniques. The ultimate objective is to determine the most effective model for ensuring deliveries are made within the company’s target timeframe."
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

### Introduction

This project aims to analyze the factors affecting the on-time delivery of orders for a major mail-order company.
By employing logistic regression, Naive-Bayes classification, and Linear Discriminant Analysis, the goal is to identify significant predictors of delivery success and compare the accuracy, processing speed, and false positive rates of these classification techniques.
The ultimate objective is to determine the most effective model for ensuring deliveries are made within the company's target timeframe.

### Data Dictionary

```{r echo=FALSE, results='asis'}
options(knitr.kable.NA = '')
data.frame(Variable = c("Del","Vin","Pkg","Cst","Mil","Dom","Haz","Car"),
           Description = c("Time for delivery (in days, rounded to nearest 10th)","Vintage of product (i.e. how long it has been in the warehouse).","How many packages of product have been ordered","How many orders the customer has made in the past","Distance the order needs to be delivered (in km)","Indicator for if the product is manufactured in Canada (C) or elsewhere (I)","Indicator for if the product is designated as Hazardous (H) or not (N).","Indicator for which Carrier delivered the item (Fed Post, or M-Press Delivery)")) |>
  knitr::kable()
```

### Initial Setup

This section is for the basic set up.
It will clear all the plots, the console and the workspace.
It also sets the overall format for numbers.

```{r echo=T, results='hide'}
if(!is.null(dev.list())) dev.off()
cat("\014") 
rm(list=ls())
options(scipen=9)
```

### Load Packages

This section loads and attaches all the necessary packages.

```{r message=FALSE, warning=FALSE}
if(!require(readxl)){install.packages("readxl")}
library("readxl")

if(!require(pastecs)){install.packages("pastecs")}
library("pastecs")

if(!require(lattice)){install.packages("lattice")}
library("lattice")

if(!require(ggplot2)){install.packages("ggplot2")}
library("ggplot2")

if(!require(tinytex)){install.packages("tinytex")}
library("tinytex")

if(!require(corrgram)){install.packages("corrgram")}
library("corrgram")

if(!require(psych)){install.packages("psych")}
library("psych")

if(!require(partykit)){install.packages("partykit")}
library("partykit")

if(!require(klaR)){install.packages("klaR")}
  library("klaR")

```

### Import Data

```{r results='hide'}
ExcelFile <- read.csv("PROG8430_Assign05_22F.txt")
ExcelFile <- as.data.frame(ExcelFile)
head(ExcelFile)
```

### Preliminary Data Preparation

First let's create a quick exploratory graphs of all variables.
Also, I have adjusted categorical variables to factor variables.

Checking if any outliners are available in any of these fields.
If available, I will remove those.

```{r}

#Box Plot and Density Plot for Time for Delivery.
boxplot(ExcelFile$Del, horizontal=TRUE, col=c("#6bc9c2"), pch=20, main = "Time of Delivery - Box Plot")
densityplot( ~ ExcelFile$Del, pch=6,col=c("#6bc9c2"), xlab = "Time of Delivery")

#Box Plot and Density Plot for Vintage of Product.
boxplot(ExcelFile$Vin, horizontal=TRUE, col=c("#6bc9c2"), pch=20, main = "Vintage of Product - Box Plot")
densityplot( ~ ExcelFile$Vin, pch=6,col=c("#6bc9c2"), xlab = "Vintage of Product")

#Box Plot and Density Plot for number of packages of product have been ordered.
boxplot(ExcelFile$Pkg, horizontal=TRUE, col=c("#6bc9c2"), pch=20, main = "No. of Product Ordered - Box Plot")
densityplot( ~ ExcelFile$Pkg, pch=6,col=c("#6bc9c2"), xlab = "No. of Product Ordered")

#Box Plot and Density Plot for number of orders the customer has made in the past.
boxplot(ExcelFile$Cst, horizontal=TRUE, col=c("#6bc9c2"), pch=20, main = "No. of Orders per Customer - Box Plot")
densityplot( ~ ExcelFile$Cst, pch=6,col=c("#6bc9c2"), xlab = "No. of Orders per Customer")

#Box Plot and Density Plot for distance the order needs to be delivered.
boxplot(ExcelFile$Mil, horizontal=TRUE, col=c("#6bc9c2"), pch=20, main = "Distance - Box Plot")
densityplot( ~ ExcelFile$Mil, pch=6,col=c("#6bc9c2"), xlab = "Distance")

#Bar Plot for Indicator for if the product is manufactured in Canada (C) or elsewhere (I).
DomBar <- table(ExcelFile$Dom)
Dom <- DomBar[order(DomBar,decreasing=TRUE)]
barplot(Dom,density = 30, angle = 45, main="Dom - Bar Plot", xlab="Dom", ylab = "Frequency", col=c("#6bc9c2"))

#Bar Plot for Indicator for if the product is designated as Hazardous (H) or not (N).
HazBar <- table(ExcelFile$Haz)
Haz <- HazBar[order(HazBar,decreasing=TRUE)]
barplot(Haz,density = 30, angle = 45, main="Haz - Bar Plot", xlab="Haz", ylab = "Frequency", col=c("#6bc9c2"))

#Bar Plot for indicator for which Carrier delivered the item.
CarBar <- table(ExcelFile$Car)
Car <- CarBar[order(CarBar,decreasing=TRUE)]
barplot(Car,density = 30, angle = 45, main="Car - Bar Plot", xlab="Car", ylab = "Frequency", col=c("#6bc9c2"))

# As we see, there are no outliners available in any of these fields.
```

```{r}
# Now, I am converting categorical variables to factor variables. 
ExcelFile <- as.data.frame(unclass(ExcelFile),stringsAsFactors = TRUE)
head(ExcelFile)

```

Creating a new variable in the dataset which will have a value of 1 if *Del\<=10* and 0 otherwise.

```{r}
ExcelFile$OT <- as.factor(ifelse(ExcelFile$Del < 10.1, 1,0))
str(ExcelFile)
```

### Exploratory Analysis

> Correlations

Creating numeric correlations and checking if there are any co-linear variables available or not?

```{r}

# Creating numeric correlation.
cor(ExcelFile[,unlist(lapply(ExcelFile,is.numeric))], method="spearman")
```

Observations on co-liner variables.

**Del** and **Mil** has strong linear relationship, with the value of 0.80.

**Vin** and **Cst** has almost no linear relationship, with the value of 0.003.

**Pkg** and **Mil** has almost no linear relationship, with a value of -0.006.

------------------------------------------------------------------------

Identifying the most significant predictor of an on time delivery and provide statistical evidence (in addition to the correlation coefficient) that suggest they are associated with an on time delivery.

```{r warning=FALSE}

# Fit logistic regression model
logit_model <- glm(OT ~ ., data = ExcelFile, family = "binomial")

# Summary of the model
summary(logit_model)

# Likelihood ratio test
anova(logit_model, test = "Chisq")

```

The logistic regression analysis indicates that distance (Mil) is the most significant predictor of on-time delivery, supported by its highly significant p-value (\< 2e-16).

Other predictors such as number of orders per customer (Cst), manufactured outside Canada (Dom), non-hazardous products (Haz), and the carrier (Car) also show strong associations with on-time delivery based on their p-values.

------------------------------------------------------------------------

### Model Development

Now, let's create two logistic regression models.

**A full model using all of the variables.**

```{r}

# Removed Del field from the data set 
# because correlation is high between newly added field OT and Del 
# as OT field is created based on Del field only.
ExcelFile <- ExcelFile[-c(1)]

#Creating a full model for all the variables.
full.model = glm(OT ~ . ,data=ExcelFile, na.action=na.omit, family="binomial")
summary(full.model)
pred <- predict(full.model, newdata=ExcelFile)

```

**An additional model using backward selection**

```{r}

# Creating another model using backward selection.
back.model = step(full.model, direction="backward", details=TRUE)
summary(back.model)
pred <- predict(back.model, newdata=ExcelFile)

```

Based on this analysis, I recommend backward model should be selected as:

Here is the several measures I have considered to choose which model is better.

> **AIC** - Model is better where AIC value is lower.

AIC value is lower in the Backward Model with the comparison of full model.

> **Deviance** - Measure of Error = null deviance - Residual deviance.

> Model is better with the higher difference value.

Deviance is higher in the full model.

> Residual symmetry value should be zero.
> Residual symmetry value is near by zero for Model 2(backward model).

> Z-values - all Pr(\>z) values less than 0.05 is consider as the better model.

For Backward model, Pr(\>z) value is less than 0.05 for all except one as 0.0816 but it's near by 0.05.

> Parameter Co-Efficient

Based on these observations, we can say that `backward model` is the better model than full model.

------------------------------------------------------------------------

> In this section, all three classifiers are built as the dependant variable and the remaining variables as the independent variables.

### Logistic Regression -- Backward

As above, used the step option in the glm function to fit the model (using backward selection).

```{r}

# Added the Start Time in the start_time.
start_time <- Sys.time()

# In regression, the dependent variable is marked as Y and the independent variable is marked as X. 
# So, here adding the OT as dependent variable and other variables as the independent variables.
glm.mod <- glm(OT ~ .,family="binomial", data=ExcelFile, na.action=na.omit)
stp.glm <- step(glm.mod)

# Added the End time in the end_time.
end_time <- Sys.time()

# Classifies
resp <- predict(back.model, type="response")
head(resp,10)

# more than 50% chance then 1 else 0.
class <- ifelse(resp > 0.5,1,0)
head(class)

```

Summarized the results in a Confusion Matrix .

```{r}

# creating the confusion matrix.
LR_CF <-table(ExcelFile$OT, class, dnn = list("Actual","Predicted"))
LR_CF

# Adding the results in confusion matrix.
LR_TP <-LR_CF[2,2]
LR_TN <-LR_CF[1,1]
LR_FP <-LR_CF[1,2]
LR_FN <-LR_CF[2,1]

```

Now, let's calculate the time (in seconds) it took to fit the model.

```{r}

# Processing time for Logistic regression
Conf_time <- end_time - start_time
Conf_time

```

### Naive-Bayes Classification

Used all the variables in the dataset to fit a Naive-Bayesian classification model.

```{r}

start_time <- Sys.time()

# In regression, the dependent variable is marked as Y and the independent variable is marked as X. 
# So, here adding the OT as dependent variable and other variables as the independent variables.
Naive <- NaiveBayes(OT ~ . ,data = ExcelFile, na.action=na.omit)

end_time <- Sys.time()

```

Summarized the results in a Confusion Matrix.

```{r warning=FALSE}
# Classifies
pred_bay <- predict(Naive,ExcelFile)
 
# Creates Confusion Matrix
Naive_CF <- table(Actual=ExcelFile$OT, Predicted=pred_bay$class)
Naive_CF

Naive_TP <-Naive_CF[2,2]
Naive_TN <-Naive_CF[1,1]
Naive_FP <-Naive_CF[1,2]
Naive_FN <-Naive_CF[2,1]

```

Now, let's calculate the time (in seconds) it took to fit the model.

```{r}

# Processing time for Naive Bayes Classification
NB_Time <- end_time - start_time
NB_Time
```

### Linear Discriminant Analysis

Used all the variables in the dataset to fit an LDA classification model.

```{r}

start_time <- Sys.time()
LDA <- lda(OT ~ .,data = ExcelFile, na.action=na.omit)
end_time <- Sys.time()

```

Summarized the results in a Confusion Matrix.

```{r}
#Classifies
pred_dis <- predict(LDA, data=ExcelFile)

#Confusion Matrix
LDA_CF <- table(Actual=ExcelFile$OT, Predicted=pred_dis$class)
LDA_CF

#Defined
LDA_TP <-LDA_CF[2,2]
LDA_TN <-LDA_CF[1,1]
LDA_FP <-LDA_CF[1,2]
LDA_FN <-LDA_CF[2,1]

```

Now, let's calculate the time (in seconds) it took to fit the model.

```{r}

# Processing time for Linear Discriminant Analysis
LDA_Time <- end_time - start_time
LDA_Time

```

> Comparing All Three Classifiers

Now take a look at Which classifier is most accurate.

```{r}
# Accuracy: The proportion of cases correctly classified: (TP+TN)/Total

# Accuracy for LR classifier.
LR_ACC <-(LR_TP+LR_TN)/sum(LR_CF)
LR_ACC

#Accuracy for Naive classifier.
Naive_ACC <-(Naive_TP+Naive_TN)/sum(Naive_CF)
Naive_ACC

#Accuracy for LDA Classifier.
LDA_ACC <-(LDA_TP+LDA_TN)/sum(LDA_CF)
LDA_ACC

# Based on the accuracy values for all three defined classifiers; 
#Linear Discriminant Analysis classifier is the most accurate because it's value is slightly higher than other two classifier's accuracy value.

```

Now let's check Which classifier is most suitable when processing speed is most important.

```{r}

# Processing time for Logistic Regression – Backward
Conf_time

# Processing time for Naive-Bayes Classification
NB_Time

# Processing time for Linear Discriminant Analysis
LDA_Time

# Processing time for Linear Discriminant Analysis is the lowest from remaining two classifiers.
# Linear Discriminant Analysis is the most suitable classifier if we consider processing speed as the most important.

```

Now let's check Which classifier minimizes false positives.

```{r}

# False Positive value for Logistic Regression Classifier 
LR_FP

# False Positive value for Naive Bayes Classification
Naive_FP

# False Positive value for Linear Discriminant Analysis Classifier
LDA_FP

# Based on the FP values for all three classifiers, 
# Logistic Regression Classifier has the lowest value for false positives.

```

**See Which classifier is best overall.**

Based on previously created classifiers, it's accuracy and False positive values, Logistic Regression classifier is the best classifier.

As Logistic Regression's value for False Positive is the lowest.

For accuracy, Logistic Regression's value is slightly lower with the comparison of Linear Discriminant Analysis Classifier with 0.001 value.

### Decision Tree

Used all the variables in the dataset to fit a Decision Tree classification model.

```{r}

start_time <- Sys.time()
tree.fit <- ctree(OT ~ ., data=ExcelFile)
end_time <- Sys.time()
plot(tree.fit, gp=gpar(fontsize=2))

```

Summarizing the results in a Confusion Matrix.

```{r}

#Classifies
pred.tree <- predict(tree.fit, ExcelFile)

# creating the confusion matrix.
CF_TREE <- table(Actual=ExcelFile$OT, Predicted=pred.tree)
CF_TREE

```

calculating the time (in seconds) it took to fit the model.

```{r}

CF_Time <- end_time - start_time
CF_Time

```